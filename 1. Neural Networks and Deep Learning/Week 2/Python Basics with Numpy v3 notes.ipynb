{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from the *Python Basics with Numpy v3* assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building basic functions with numpy\n",
    "\n",
    "### 1.1. sigmoid function, np.exp()\n",
    "\n",
    "The sigmoid function is sometimes known as the logistic function. It is used in machine learning for logistic regression, and also in deep learning. \n",
    "\n",
    "![sigmoid.png](./sigmoid.png?raw=true)\n",
    "\n",
    "In deep learning we mostly use matrices and vectors, so the numpy library is more useful than the math library. Instead of math.exp(), we want to use np.exp() as it applies the exponential function to every element of z. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([1, 2, 3])\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Sigmoid gradient\n",
    "\n",
    "We compute gradients to optimize loss functions using backpropagation. \n",
    "\n",
    "This function computes the gradient of the sigmoid function with respect to its input z. \n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Find the sigmoid of z\n",
    "z = np.array([1, 2, 3])\n",
    "s = sigmoid(z)\n",
    "\n",
    "# Step 2. Compute the sigmoid derivative\n",
    "ds = s * (1 - s)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Reshaping arrays\n",
    "\n",
    "Two common functions used in deep learning:\n",
    "\n",
    "- `np.shape` — used to get the shape (dimension) of a matrix/vector \n",
    "- `np.reshape()` — used to reshape a matrix/vector into some other dimension\n",
    "\n",
    "To unroll, or reshape a 3D array into a 1D vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.67826139 0.29380381]\n",
      "  [0.90714982 0.52835647]\n",
      "  [0.4215251  0.45017551]]\n",
      "\n",
      " [[0.92814219 0.96677647]\n",
      "  [0.85304703 0.52351845]\n",
      "  [0.19981397 0.27417313]]\n",
      "\n",
      " [[0.60659855 0.00533165]\n",
      "  [0.10820313 0.49978937]\n",
      "  [0.34144279 0.94630077]]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize 3D array  \n",
    "arr = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions:  (3, 3, 2) \n",
      "\n",
      "[[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# Get dimensions\n",
    "print(\"Dimensions: \", arr.shape, \"\\n\")\n",
    "\n",
    "# Reshape the 3D array into 1D vector using its dimensions\n",
    "dimensions = 1\n",
    "for i in range(0, len(arr.shape)):\n",
    "    dimensions *= arr.shape[i]\n",
    "reshaped_arr = arr.reshape(dimensions, 1)\n",
    "print(reshaped_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Normalizing rows\n",
    "\n",
    "When we normalize our data, we divide each row vector of matrix x by its norm.\n",
    "\n",
    "For example, if \n",
    "\n",
    "$$x = \\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "then the norm of each row is \n",
    "\n",
    "$$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and x normalized is \n",
    "\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Implement normalization of matrix x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 4]\n",
      " [1 6 4]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize matrix\n",
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the rows of the matrix\n",
    "x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "x = x/x_norm\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Broadcasting and the softmax function\n",
    "\n",
    "Broadcasting documentation: https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "\n",
    "An important concept in numpy, broadcasting is useful for performing mathematical operations between arrays of different shapes. We briefly discussed this in the first notebook from this week. \n",
    "\n",
    "Softmax is a function that can be used to normalize a matrix. More about softmax: https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "The function is\n",
    "\n",
    "$$\\frac{e^{x_n}}{\\sum_{j}e^{x_j}}$$\n",
    "\n",
    "where n is the number of features (or columns) and j is the number of instances (or rows).\n",
    "\n",
    "Calculate the softmax for each row of matrix x, which should automatically use numpy broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 2 5 0 0]\n",
      " [7 5 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize matrix\n",
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0, 0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.10308393e+03 7.38905610e+00 1.48413159e+02 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [1.09663316e+03 1.48413159e+02 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]]\n",
      "Shape of the softmax numerator:  (2, 5)\n"
     ]
    }
   ],
   "source": [
    "# Apply exp() to each element of x\n",
    "x_exp = np.exp(x)\n",
    "print(x_exp)\n",
    "print(\"Shape of the softmax numerator: \", x_exp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8260.88614278]\n",
      " [1248.04631753]]\n",
      "Shape of the softmax denominator:  (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Sum each row of x_exp\n",
    "x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "print(x_sum)\n",
    "print(\"Shape of the softmax denominator: \", x_sum.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we divide the two matrices to compute the softmax of each row, numpy automatically applies broadcasting to expand the shape of the softmax denominator into (2, 5), to make this division possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Compute softmax of each row\n",
    "s = x_exp / x_sum\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "### 2.1. Implement the L1 and L2 loss functions\n",
    "\n",
    "In deep learning, we use optimization algorithms like gradient descent to train our model and to minimize the loss/cost. The loss is used to evaluate the performance of our model. The bigger the loss, the more different the predictions (ŷ) are from the true values (y). \n",
    "\n",
    "L1 loss is defined as\n",
    "\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n"
     ]
    }
   ],
   "source": [
    "# Initialize predicted values and true values\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "# Implement the numpy vectorized version of the L1 loss\n",
    "loss = np.sum(np.abs(y - yhat))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 loss is defined as\n",
    "\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43\n"
     ]
    }
   ],
   "source": [
    "# Implement the numpy vectorized version of the L2 loss\n",
    "loss = np.sum(np.power((y - yhat), 2))\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
