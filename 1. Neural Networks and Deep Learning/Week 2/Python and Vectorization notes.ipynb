{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from the *Python and Vectorization* video series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, numpy as np, math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Vectorization\n",
    "\n",
    "Compare vectorized and non-vectorized operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250055.0846118524\n",
      "Non-vectorized version: 890.1820182800293ms\n",
      "\n",
      "250055.0846118598\n",
      "Vectorized version: 3.3097267150878906ms\n"
     ]
    }
   ],
   "source": [
    "# Initialize two random arrays\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "# Test non-vectorized version \n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i] * b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"Non-vectorized version: \" + str(1000*(toc-tic)) + \"ms\\n\")\n",
    "\n",
    "# Test vectorization version \n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explicit for loop and non-vectorized version took about 890.18ms. The vectorized version took approx. 3.31ms. \n",
    "\n",
    "Vectorization can significantly speed up your code. Try to avoid explicit for loops whenever possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: More Vectorization Examples\n",
    "\n",
    "Say you need to apply the exponential operation on every element of a matrix/vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-vectorized version: 1229.051113128662ms\n",
      "Vectorized version: 14.938116073608398ms\n"
     ]
    }
   ],
   "source": [
    "# Initialize random array \n",
    "v = np.random.rand(1000000)\n",
    "n = len(v)\n",
    "\n",
    "# Vectorized version\n",
    "u = np.zeros((n, 1))\n",
    "tic = time.time()\n",
    "for i in range(n):\n",
    "    u[i] = math.exp(v[i])\n",
    "toc = time.time()\n",
    "print(\"Non-vectorized version: \" + str(1000*(toc-tic)) + \"ms\")\n",
    "\n",
    "# Non-vectorized version\n",
    "tic = time.time()    \n",
    "u = np.exp(v)\n",
    "toc = time.time()\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the vectorized version is much faster.\n",
    "\n",
    "The NumPy library has more built-in functions to compute vector operations, such as:\n",
    "- `np.log()`\n",
    "- `np.abs()`\n",
    "- `np.maximum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Vectorizing Logistic Regression\n",
    "\n",
    "**How to perform logistic regression without explicit for loops:**\n",
    "\n",
    "In our logistic regression gradient descent example earlier this week, we wrote 2 explicit for loops, the first of which propagates forward through the neural net. It iterates over m training examples to calculate the prediction a (or ŷ) of that training example with the help of a subfunction z. \n",
    "\n",
    "Original, highly inefficient non-vectorized logistic regression implementation: \n",
    "\n",
    "```\n",
    "J = 0\n",
    "dw1 = 0\n",
    "dw2 = 0\n",
    "db = 0\n",
    "\n",
    "for i in range(1, m):\n",
    "    z = (wT * x) + b\n",
    "    a = σ(z)\n",
    "    J += -[(y * log a) + ((1 - y) * log(1 - a))]\n",
    "    dz = a - y\n",
    "    dw1 += x1 * dz\n",
    "    dw2 += x2 * dz\n",
    "    db += dz\n",
    "\n",
    "J = J/m\n",
    "dw1 = dw1/m\n",
    "dw2 = dw2/m\n",
    "db = db/m\n",
    "```\n",
    "\n",
    "However, instead of doing the first for loop, we can run \n",
    "\n",
    "```\n",
    "Z = np.dot(wT, x) + b\n",
    "A = σ(Z)\n",
    "```\n",
    "\n",
    "to compute all of the z's and a's at the same time. The output of Z and A are 1xm vectors containing one z or a for each example. \n",
    "\n",
    "*Note: The T in wT denotes its transpose. It is supposed to be shown as a superscript.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Vectorizing Logistic Regression's Gradient Computation\n",
    "\n",
    "**How to use vectorization to also perform gradient descent:**\n",
    "\n",
    "Continuing from the last video, with dw1 and dw2, we see the second for loop, which is part of the backward propagation through the neural net. It iterates over n features to get the derivatives (or slopes) of each feature of that training example. Here n = 2, so we have dw1 and dw2. If n = 5, we would have dw1 through dw5. \n",
    "\n",
    "To vectorize this for loop, we replace it with \n",
    "\n",
    "```\n",
    "dw += x * dz\n",
    "```\n",
    "\n",
    "Putting together our vectorized for loops from the last video and this video, the updated full implementation is\n",
    "\n",
    "```\n",
    "Z = np.dot(wT, x) + b\n",
    "A = σ(Z)\n",
    "dZ = A - Y\n",
    "dw = 1/m * dzT\n",
    "db = 1/m * np.sum(dZ)\n",
    "```\n",
    "\n",
    "We've now done front propagation and back propagation, computing the predictions and derivatives on all m training examples without using a full loop.\n",
    "\n",
    "And after we update the weight and loss variables, \n",
    "\n",
    "```\n",
    "w = w - (α * dw)\n",
    "b = b - (α * db)\n",
    "```\n",
    "\n",
    "we've implemented a single iteration of gradient descent for logistic regression. \n",
    "\n",
    "*Note: α, or alpha, is the learning rate.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: Broadcasting in Python\n",
    "\n",
    "Broadcasting is a technique that Python and numpy use to allow you to make certain parts of your code much more efficient, e.g. removing for loops. \n",
    "\n",
    "Take this matrix:\n",
    "\n",
    "![broadcasting_in_python.png](./broadcasting_in_python.png?raw=true)\n",
    "\n",
    "Without explicit looping, we can add up the columns to find the total number of calories for apples, beef, eggs, and potatoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56.    0.    4.4  68. ]\n",
      " [  1.2 104.   52.    8. ]\n",
      " [  1.8 135.   99.    0.9]]\n"
     ]
    }
   ],
   "source": [
    "# Intialize the matrix\n",
    "A = np.array([[56.0, 0.0, 4.4, 68.0],\n",
    "              [1.2, 104.0, 52.0, 8.0],\n",
    "              [1.8, 135.0, 99.0, 0.9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 59.  239.  155.4  76.9]\n"
     ]
    }
   ],
   "source": [
    "# Sum each column\n",
    "cal = A.sum(axis=0) \n",
    "print(cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of calories for apples, beef, eggs, potatoes is 59, 239, 155.4, and 76.9, respectively.\n",
    "\n",
    "*Note: With axis = 0 we are specifying we want to operate on columns, whereas axis = 1 is rows.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94.91525424  0.          2.83140283 88.42652796]\n",
      " [ 2.03389831 43.51464435 33.46203346 10.40312094]\n",
      " [ 3.05084746 56.48535565 63.70656371  1.17035111]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of carb, protein, fat in each food item\n",
    "percentage = 100 * A/cal\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94.9% of the calories in apples are from carbs, and so on.\n",
    "\n",
    "We had a 3x4 matrix (`A`) and we divided it by a 1x4 matrix (`cal`). How is this possible? Through broadcasting.\n",
    "\n",
    "For example, if you take a 4x1 vector, such as `[1, 2, 3, 4]`, and add it to a number, say 100, what Python will do is take this number and auto-expand it into a 4x1 vector as well, so it becomes `[100, 100, 100, 100]`, and then the sum is `[101, 102, 103, 104]`.\n",
    "\n",
    "As another example, if we have (m, n) and (1, n) matrices\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{@{}ccc@{}}\n",
    "                    1 & 2 & 3 \\\\\n",
    "                    4 & 5 & 6\\end{array}\\right) + \\left(\\begin{array}{@{}ccc@{}}\n",
    "                    100 & 200 & 300 \\end{array}\\right)$$\n",
    "                    \n",
    "the 1xn matrix gets expanded into (m, n) dimensions, and the sum is \n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{@{}ccc@{}}\n",
    "                    101 & 202 & 303 \\\\\n",
    "                    104 & 205 & 306\\end{array}\\right)$$\n",
    "                   \n",
    "We did broadcasting earlier with the constant b in logistic regression, using a division operation.\n",
    "\n",
    "Broadcasting documentation: https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video: A note on python/numpy vectors\n",
    "\n",
    "Tips and tricks to avoid strange bugs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15568529 0.82572763 0.57840157 0.65935008 0.1244858 ]\n"
     ]
    }
   ],
   "source": [
    "# Create 5 random Gaussian variables stored in an array\n",
    "a = np.random.randn(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Not a vector",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-35a69bb13459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Not a vector\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Not a vector"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "assert a.shape == (5,1), \"Not a vector\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape indicates that a is a rank 1 array in Python and is neither a matrix nor a row vector. This has some slightly non-intuitive effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.380593   -0.6866715  -1.89812336 -1.43680426  0.2762907 ]\n"
     ]
    }
   ],
   "source": [
    "# Print a-transpose\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transpose looks the same as a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.121170129144351\n"
     ]
    }
   ],
   "source": [
    "# Print the inner product between a and a-transpose\n",
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might think that a times a tranpose would give us a matrix. But we get back a number. \n",
    "\n",
    "When coding neural networks, don't use data structures that are not vectors or matrices. They will not behave like we expect, because they simply are not vectors or matrices.\n",
    "\n",
    "Instead, tell Python we want a column vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8660183 ]\n",
      " [-1.48532738]\n",
      " [-2.25084207]\n",
      " [ 0.95650147]\n",
      " [ 0.87325993]]\n"
     ]
    }
   ],
   "source": [
    "# Create 5 random Gaussian variables stored in a column vector\n",
    "a = np.random.randn(5,1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert a.shape == (5,1), \"Not a vector\" # Does not throw an AssertionError because it is a vector now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22346984  0.03112995  0.35188154  0.62692364  0.66043587]]\n"
     ]
    }
   ],
   "source": [
    "# Print a-transpose\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are two square brackets when creating a transpose of a 1x5 matrix. \n",
    "\n",
    "If we do end up with a rank 1 array, we can reshape it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9396036  -0.71911714 -1.17814407  1.14204755  0.23493005]\n",
      "[[-0.9396036 ]\n",
      " [-0.71911714]\n",
      " [-1.17814407]\n",
      " [ 1.14204755]\n",
      " [ 0.23493005]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the rank 1 array into a vector\n",
    "a = np.random.randn(5)\n",
    "print(a)\n",
    "a = a.reshape(5, 1)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
